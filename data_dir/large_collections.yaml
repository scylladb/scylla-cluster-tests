keyspace: large_collection_test

keyspace_definition: |

  CREATE KEYSPACE large_collection_test WITH replication = {'class': 'NetworkTopologyStrategy', 'replication_factor': '3'}  AND durable_writes = true;

table: table_with_large_collection

table_definition: |

  CREATE TABLE large_collection_test.table_with_large_collection (
    pk_id text,
    init_time timestamp,
    user_name text,
    new_num int,
    weight decimal,
    nums_list list<int>,
    users_name list<text>,
    nums_set set<int>,
    names_set set<text>,
    books_set set<text>,
    static_data1 blob static,
    static_data2 blob static,
    PRIMARY KEY(pk_id, user_name)
  ) WITH bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'ALL'}
    AND comment = 'Request for unit level UIs'
    AND compaction = {'class': 'SizeTieredCompactionStrategy'}
    AND compression = {'sstable_compression': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99.0PERCENTILE';

columnspec:
  - name: pk_id
    population: seq(1..8000000)

# Need to populate the list field with big elements and many of them (about 40MB per list).
# >>> math.sqrt(40 * 1024 * 1024)
# 6476.344648024841
# large partition: 16M, large row: 4M, large cell: 1M
# >>> math.sqrt(16 * 1024 * 1024)
# 4096.0
# >>> math.sqrt(1024 * 1024 * 15.5 / 2)
# 2850.695353768971
# >>> math.sqrt(4 * 1024 * 1024)
# 2048.0
# >>> math.sqrt(1024 * 1024 * 4 / 2)
# 1448.1546878700494
# >>> math.sqrt(1 * 1024 * 1024)
# 1024.0

# gaussian(min..max,mean,stdev)
# The large collections threshold is 10,000
# Described here: https://github.com/scylladb/scylladb/commit/20bad62562268bed1a78a0f0466c62359565f0c0
# gaussian(6000..12000,9000,1000) gives us 15% probability above the threshold (10,000)


  - name: nums_list
    size: gaussian(6000..12000,9000,1000)
    population: uniform(1..100)

  - name: users_name
    size: fixed(5)

  - name: nums_set
    size: gaussian(6000..12000,9000,1250)
    population: uniform(1..100)

  - name: names_set
    size: fixed(1024)
    population: uniform(1..1024)

  - name: books_set
    size: fixed(1024)
    population: uniform(1..1024)

  - name: static_data1
    size: uniform(2620..62400)

  - name: static_data2
    size: uniform(2620..62400)


insert:
  batchtype: UNLOGGED

queries:
  read1:
    cql: select * from large_collection_test.table_with_large_collection where pk_id = ? and user_name = ?
    fields: samerow
  update1:
    cql: update large_collection_test.table_with_large_collection set users_name=?,nums_list=?,nums_set=?,names_set=?,books_set=?, static_data1=?, static_data2=? where pk_id = ? and user_name = ?
    fields: samerow
