{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9402cec",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a8c19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import pprint\n",
    "from sdcm.utils.raft.common import get_topology_coordinator_node\n",
    "from sdcm.utils.nemesis_utils.node_operations import is_node_seen_as_down\n",
    "from sdcm.wait import wait_for\n",
    "from sdcm.utils.raft.common import get_topology_coordinator_node\n",
    "from sdcm import sct_abs_path\n",
    "from sdcm.nemesis import Nemesis\n",
    "from sdcm.sct_config import SCTConfiguration\n",
    "from sdcm.utils.common import ParallelObject\n",
    "\n",
    "from longevity_test import LongevityTest\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1f12a7",
   "metadata": {},
   "source": [
    "### Cluster configuration \n",
    "better to use Env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac783b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"BUILD_USER_EMAIL\"] = \"alex.bykov@scylladb.com\"\n",
    "os.environ[\"BUILD_ID\"] = \"555\"\n",
    "os.environ[\"SCT_CLUSTER_BACKEND\"] = \"aws\"\n",
    "os.environ[\"SCT_CONFIG_FILES\"] = \"test-cases/longevity/longevity-100gb-4h.yaml\"\n",
    "os.environ[\"SCT_SCYLLA_VERSION\"] = \"\"\n",
    "# os.environ[\"SCT_AMI_ID_DB_SCYLLA\"] = \"ami-0b7362e1db3807a21\"  # custom ami with mv building\n",
    "os.environ[\"SCT_SCYLLA_VERSION\"] = \"master:latest\"\n",
    "os.environ[\"SCT_REGION_NAME\"] = \"eu-west-1 us-east-1\"\n",
    "os.environ[\"SCT_AVAILABILITY_ZONE\"] = \"a\"\n",
    "os.environ[\"SCT_USE_MGMT\"] = \"False\"\n",
    "os.environ[\"SCT_N_DB_NODES\"] = \"4 4\"\n",
    "os.environ[\"SCT_SIMULATED_RACKS\"] = \"2\"\n",
    "os.environ[\"SCT_N_LOADERS\"] = \"1\"\n",
    "os.environ[\"SCT_N_MONITORS_NODES\"] = \"1\"\n",
    "os.environ[\"SCT_INSTANCE_PROVISION\"] = \"on_demand\"\n",
    "os.environ[\"SCT_ENABLE_ARGUS\"] = \"False\"\n",
    "os.environ['SCT_INSTANCE_TYPE_DB'] = 'i4i.2xlarge'\n",
    "os.environ[\"SCT_NEMESIS_CLASS_NAME\"] = \"NoOpMonkey\"\n",
    "os.environ[\"SCT_STRESS_CMD\"] = \"\"\n",
    "os.environ['SCT_PREPARE_WRITE_CMD'] = \"\"\n",
    "os.environ['SCT_IP_SSH_CONNECTIONS'] = 'public'\n",
    "\n",
    "assert os.environ[\"BUILD_USER_EMAIL\"] != \"sct.tester@scylladb.com\", \"please use your own email so resources are tracked properly\"\n",
    "\n",
    "# logging configuration, for jupyter only (sct logs are intact)\n",
    "import logging\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "LOGGER.setLevel(logging.DEBUG)\n",
    "consoleHandler = logging.StreamHandler()\n",
    "consoleHandler.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "consoleHandler.setFormatter(formatter)\n",
    "LOGGER.addHandler(consoleHandler)\n",
    "\n",
    "sct_config = SCTConfiguration()\n",
    "sct_config.verify_configuration()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b6c0ee",
   "metadata": {},
   "source": [
    "### Define additional functions\n",
    "Add several functions for node operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b035777a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import DefaultDict\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class VoterState:\n",
    "    name: str\n",
    "    ip_address: str\n",
    "    host_id: str\n",
    "    is_voter: bool\n",
    "    \n",
    "\n",
    "def get_current_voters_states(nodes: list[\"BaseNode\"], verification_node: \"BaseNode\"):\n",
    "    voters_state = DefaultDict()\n",
    "    group0_members = verification_node.raft.get_group0_members()\n",
    "    nodes = nodes or verification_node.parent_cluster.get_nodes_up_and_normal(verification_node)\n",
    "    hostid_node_map = {node.host_id: node for node in nodes}\n",
    "    for member in group0_members:\n",
    "        if node := hostid_node_map.get(member['host_id']):\n",
    "            voters_state.setdefault(node.region, []).append(\n",
    "                VoterState(node.name, node.ip_address,  member['host_id'], member['voter']))\n",
    "    LOGGER.debug(\"Voters per region: %s\", voters_state)\n",
    "    return voters_state\n",
    "\n",
    "\n",
    "def stop_scylla_server_not_gently(node: \"BaseNode\"):\n",
    "    node.remoter.sudo(\"sed -i 's/Restart=on-abnormal/#Restart=on-abnormal/' /usr/lib/systemd/system/scylla-server.service\")\n",
    "    node.remoter.sudo(\"sudo systemctl daemon-reload\")\n",
    "    node.remoter.sudo(\"pkill -9 scylla\")\n",
    "\n",
    "\n",
    "def replace_cluster_node(cluster, verification_node: \"BaseNode\",\n",
    "                            host_id: str | None = None,\n",
    "                            dc_idx: int = 0,\n",
    "                            rack: int = 0,\n",
    "                            ignore_dead_node_host_ids: str = \"\",\n",
    "                            timeout: int | float = 3600 * 8) -> \"BaseNode\":\n",
    "    \"\"\"When old_node_ip or host_id are not None then replacement node procedure is initiated\"\"\"\n",
    "    cluster.log.info(\"Adding new node to cluster...\")\n",
    "    new_node: \"BaseNode\" = cluster.add_nodes(count=1, dc_idx=dc_idx, rack=rack, enable_auto_bootstrap=True)[0]\n",
    "    cluster.monitor.reconfigure_scylla_monitoring()\n",
    "    new_node.remoter.sudo(f\"\"\"echo 'ignore_dead_nodes_for_replace: {ignore_dead_node_host_ids}' | sudo tee -a  /etc/scylla/scylla.yaml\"\"\")\n",
    "    new_node.replacement_host_id = host_id\n",
    "\n",
    "    try:\n",
    "        cluster.wait_for_init(node_list=[new_node], timeout=timeout, check_node_health=False)\n",
    "        cluster.clean_replacement_node_options(new_node)\n",
    "        cluster.set_seeds()\n",
    "        cluster.update_seed_provider()\n",
    "    except Exception:\n",
    "        cluster.log.warning(\"TestConfig of the '%s' failed, removing it from list of nodes\" % new_node)\n",
    "        cluster.nodes.remove(new_node)\n",
    "        cluster.log.warning(\"Node will not be terminated. Please terminate manually!!!\")\n",
    "        raise\n",
    "\n",
    "    cluster.wait_for_nodes_up_and_normal(nodes=[new_node], verification_node=verification_node)\n",
    "    new_node.wait_node_fully_start()\n",
    "    new_node.remoter.sudo(f\"\"\"sed -i 's/ignore_dead_nodes_for_replace: {ignore_dead_node_host_ids}/# ignore_dead_nodes_for_replace:/' /etc/scylla/scylla.yaml\"\"\")\n",
    "\n",
    "    return new_node\n",
    "\n",
    "\n",
    "def replace_nodes_by_host_id(cluster, dead_node_mapping: dict[str, \"BaseNode\"], verification_node: \"BaseNode\"):\n",
    "    host_ids = list(dead_node_mapping.keys())\n",
    "    new_nodes = []\n",
    "    for i, host_id in enumerate(host_ids):\n",
    "        cluster.log.info(\"Replace node %s with host_id: %s\", dead_node_mapping[host_id].name, host_id)\n",
    "        new_nodes.append(replace_cluster_node(cluster, verification_node,\n",
    "                                                    dc_idx=dead_node_mapping[host_id].dc_idx,\n",
    "                                                    rack=dead_node_mapping[host_id].rack,\n",
    "                                                    host_id=host_id,\n",
    "                                                    ignore_dead_node_host_ids=\",\".join(host_ids[i+1:])))\n",
    "\n",
    "    for node in dead_node_mapping.values():\n",
    "        cluster.terminate_node(node)\n",
    "\n",
    "    cluster.wait_all_nodes_un()\n",
    "\n",
    "\n",
    "def add_node_to_dc(cluster, dc_idx: int = 0, rack: int = 0) -> \"BaseNode\":\n",
    "    cluster.log.info(\"Adding new node\")\n",
    "    new_node = cluster.add_nodes(1, dc_idx=dc_idx, rack=rack, enable_auto_bootstrap=True)[0]\n",
    "    cluster.wait_for_init(node_list=[new_node], timeout=900,\n",
    "                                    check_node_health=True)\n",
    "    cluster.wait_for_nodes_up_and_normal(nodes=[new_node])\n",
    "    cluster.monitor.reconfigure_scylla_monitoring()\n",
    "\n",
    "    return new_node\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cc47d3",
   "metadata": {},
   "source": [
    "### Initialize clsuter\n",
    "Create a cluster and add 2 new attribures for easy usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7497a899",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdcm.sct_events.setup import start_events_device\n",
    "\n",
    "\n",
    "LOGGER.info(\"Start cluster init\")\n",
    "os.chdir(sct_abs_path(relative_filename=\"\"))\n",
    "tester_inst = LongevityTest()\n",
    "tester_inst.setUpClass()\n",
    "tester_inst._init_logging()\n",
    "start_events_device(log_dir=tester_inst.logdir,\n",
    "                            _registry=getattr(tester_inst, \"_registry\", None) or tester_inst.events_processes_registry)\n",
    "tester_inst.setUp()\n",
    "cluster = tester_inst.db_cluster\n",
    "setattr(cluster, \"monitor\", tester_inst.monitors)\n",
    "setattr(cluster, \"loaders\", tester_inst.loaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf42184f",
   "metadata": {},
   "source": [
    "### fill db with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259705e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_inst.params[\"prepare_write_cmd\"] = \"cassandra-stress write cl=ALL n=10000000  -schema 'replication(strategy=NetworkTopologyStrategy,replication_factor=2)' -mode cql3 native -rate threads=100 -col 'size=FIXED(1024) n=FIXED(10)' -pop seq=1..10000000 -log interval=15\"\n",
    "tester_inst.run_prepare_write_cmd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff432d1",
   "metadata": {},
   "source": [
    "## Define required variables\n",
    "\n",
    "with limited voters, need to find dc wilt most number of voters with next kill it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102234be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "voters_status = get_current_voters_states(cluster.nodes, cluster.nodes[0])\n",
    "alive_dc_region, dead_dc_region = sorted(voters_status, key=lambda region: len(list(filter(lambda v: v.is_voter, voters_status[region]))))\n",
    "nodes_by_region = cluster.nodes_by_region(cluster.nodes)\n",
    "host_id_node_map = {node.host_id: node for node in cluster.nodes}\n",
    "dead_node_host_ids_maps = {node.host_id: node for node in nodes_by_region[dead_dc_region]}\n",
    "dead_node_host_ids = list(dead_node_host_ids_maps.keys())\n",
    "verification_node = nodes_by_region[alive_dc_region][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99313b3",
   "metadata": {},
   "source": [
    "### Stop all nodes in DC with largest number of voters.\n",
    "\n",
    "Need stop them very fast better in parallel, so voters was not reassigned to another dc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37a5997",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parallel_stop = ParallelObject(nodes_by_region[dead_dc_region], num_workers=len(nodes_by_region[dead_dc_region]), timeout = 600)\n",
    "parallel_stop.run(stop_scylla_server_not_gently)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127443ea",
   "metadata": {},
   "source": [
    "# GET CLUSTER STATUS\n",
    "\n",
    "This cell could be used to validate current status of cluster with `nodetool status`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58689968",
   "metadata": {},
   "outputs": [],
   "source": [
    "verification_node = nodes_by_region[alive_dc_region][0]\n",
    "print(verification_node.run_nodetool(\"status\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de494749",
   "metadata": {},
   "source": [
    "### Validate that raft quorum lost\n",
    "Try to remove node dead node\n",
    "Call read barrier.\n",
    "\n",
    "Both operations should failed with exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c479fbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    verification_node.run_nodetool(f\"removenode {dead_node_host_ids[0]} --ignore-dead-nodes {','.join(dead_node_host_ids[1:])}\")\n",
    "except Exception as exc:\n",
    "    print(f\"Expected to fail. Error: {exc}\")\n",
    "\n",
    "try:\n",
    "    verification_node.raft.call_read_barrier()\n",
    "except Exception as exc:\n",
    "    print(f\"Expected to fail. Error: {exc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571d6f73",
   "metadata": {},
   "source": [
    "# Recovery procedure\n",
    "Execute each step one by one\n",
    "Use 'GET CLUSTER STATUS' cell to check cluster status between steps if need"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f632964f",
   "metadata": {},
   "source": [
    "## Step 2.\n",
    "Stop one of alive node in alive dc.\n",
    "Assume that it is the latest node in list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1893f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopped_alive_node = nodes_by_region[alive_dc_region][-1]\n",
    "stopped_node_host_id = stopped_alive_node.host_id\n",
    "stopped_alive_node.stop_scylla()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c8a1ce",
   "metadata": {},
   "source": [
    "## Step 4.\n",
    "Choose alive nodes which will be used for cluster restore\n",
    "Rolling restart of alive nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42acf42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_alive_nodes = nodes_by_region[alive_dc_region][:-1]\n",
    "cluster.restart_scylla(nodes=base_alive_nodes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fc3063",
   "metadata": {},
   "source": [
    "## Step 6. \n",
    "Find recovery coordinator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5c9943",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = verification_node.run_cqlsh(\"select value from system.scylla_local where key = 'raft_group0_id'\", split=True)\n",
    "group0_id = result[3]\n",
    "commit_idx = 0\n",
    "recovery_coordinator = None\n",
    "for node in base_alive_nodes:\n",
    "    cidx = verification_node.run_cqlsh(f\"select commit_idx from system.raft where group_id = {group0_id}\", split=True)[3]\n",
    "    if int(cidx) > commit_idx:\n",
    "        commit_idx = int(cidx)\n",
    "        recovery_coordinator = node\n",
    "\n",
    "print(recovery_coordinator.name, recovery_coordinator.ip_address, recovery_coordinator.host_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f076d21c",
   "metadata": {},
   "source": [
    "## Step 7. \n",
    "Remove raft_group_id from system.scylla_local and truncate system.discovery on each live node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8156024",
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in base_alive_nodes:\n",
    "    node.run_cqlsh(\"DELETE value FROM system.scylla_local WHERE key = 'raft_group0_id'\")\n",
    "    node.run_cqlsh(\"TRUNCATE system.discovery\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed22247",
   "metadata": {},
   "source": [
    "## Step 8.\n",
    "Set the new scylla.yaml parameter, recovery_leader, to Host ID of the recovery leader on each live node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cc628e",
   "metadata": {},
   "outputs": [],
   "source": [
    "recovery_host_id = recovery_coordinator.host_id\n",
    "for node in base_alive_nodes:\n",
    "    node.remoter.sudo(f\"\"\"echo 'recovery_leader: {recovery_host_id}' | sudo tee -a /etc/scylla/scylla.yaml\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f23e58",
   "metadata": {},
   "source": [
    "## Step 9.\n",
    "Rolling restart all live nodes, but the recovery leader must be restarted first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545d32ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cluster.restart_scylla(nodes=[recovery_coordinator])\n",
    "cluster.restart_scylla(nodes=[node for node in base_alive_nodes if node != recovery_coordinator])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4691849",
   "metadata": {},
   "source": [
    "## Step 10.\n",
    "Remove/replace all dead nodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617dc05e",
   "metadata": {},
   "source": [
    "### step 10.1\n",
    "Remove stopped node in alive dc. it is possible because each rack had 2 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22807189",
   "metadata": {},
   "outputs": [],
   "source": [
    "verification_node.run_nodetool(f\"removenode {stopped_node_host_id} --ignore-dead-nodes {','.join(dead_node_host_ids)}\")\n",
    "cluster.nodes.remove(host_id_node_map[stopped_node_host_id])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f56cae",
   "metadata": {},
   "source": [
    "### step 10.2\n",
    "Replace all nodes in dead dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc25935",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "replace_nodes_by_host_id(cluster, dead_node_host_ids_maps, verification_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b04413",
   "metadata": {},
   "source": [
    "#### Step 10.3\n",
    "Add new nodes to racks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597a06c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_idx = cluster.params.region_names.index(dead_dc_region)\n",
    "for rack in range(2):\n",
    "    new_node = add_node_to_dc(cluster, dc_idx, rack)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0bc430",
   "metadata": {},
   "source": [
    "### 10.4\n",
    "Add new node instead of remove node in alive dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f64b356",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_idx = cluster.params.region_names.index(alive_dc_region)\n",
    "new_node_instead_of_stopped = add_node_to_dc(cluster, dc_idx, rack=stopped_alive_node.rack)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edba70d7",
   "metadata": {},
   "source": [
    "## Step 11.\n",
    "Unset recovery_leader on all nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adb384c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in base_alive_nodes:\n",
    "    node.remoter.sudo(f\"\"\"sed -i  's/recovery_leader:.*/#recovery_leader:/' /etc/scylla/scylla.yaml\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8be2d2",
   "metadata": {},
   "source": [
    "## Step 12.\n",
    "Delete data of the old group 0 from system.raft, system.raft_snaphots, and system.raft_snapshot_config.\n",
    "\n",
    "```CQL\n",
    "DELETE FROM system.raft WHERE group_id = <old group 0 id>\n",
    "DELETE FROM system.raft_snapshots WHERE group_id = <old group 0 id>\n",
    "DELETE FROM system.raft_snapshot_config WHERE group_id = <old group 0 id>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae2b6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in base_alive_nodes:\n",
    "    node.run_cqlsh(f\"DELETE FROM system.raft WHERE group_id = {group0_id}\")\n",
    "    node.run_cqlsh(f\"DELETE FROM system.raft_snapshots WHERE group_id = {group0_id}\")\n",
    "    node.run_cqlsh(f\"DELETE FROM system.raft_snapshot_config WHERE group_id = {group0_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3200ecc3",
   "metadata": {},
   "source": [
    "## Get cluster status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f8ba3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "verification_node = nodes_by_region[alive_dc_region][0]\n",
    "print(verification_node.run_nodetool(\"status\"))\n",
    "cluster.check_nodes_up_and_normal(verification_node=verification_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb93f682",
   "metadata": {},
   "source": [
    "### Clean the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70774d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdcm.sct_events.setup import stop_events_device\n",
    "\n",
    "tester_inst.params['post_behavior_db_nodes'] = 'destroy'\n",
    "tester_inst.params['post_behavior_loader_nodes'] = 'destroy'\n",
    "tester_inst.params['post_behavior_monitor_nodes'] = 'destroy'\n",
    "tester_inst.params['execute_post_behavior'] = True\n",
    "\n",
    "tester_inst.clean_resources()\n",
    "stop_events_device()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
